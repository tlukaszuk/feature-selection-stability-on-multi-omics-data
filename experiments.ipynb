{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {}\n",
    "for name in [\"BLCA\", \"BRCA\", \"COAD\", \"ESCA\", \"HNSC\", \"LGG\", \"LIHC\", \"LUAD\", \"LUSC\", \"PAAD\", \"PRAD\", \"SARC\", \"SKCM\", \"STAD\", \"UCEC\"]:\n",
    "    df = pd.read_csv(f'datasets/{name}.csv')\n",
    "    datasets[name] = df\n",
    "    vc = df.iloc[:,0].value_counts().sort_index()\n",
    "    print(f\"{name}\\t: {df.shape}\\t{list(vc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset utils functions\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def omics_str(omics):\n",
    "    if type(omics) == str:\n",
    "        omics = set([omics])\n",
    "    return reduce(lambda a,b: a+\"_\"+b, omics)\n",
    "\n",
    "def get_X(df, omics):\n",
    "    if type(omics) == str:\n",
    "        if omics == \"__ALL__\":\n",
    "            return df.values\n",
    "        omics = set([omics])\n",
    "    indexes = [\n",
    "        i\n",
    "        for i, name in enumerate(df.columns)\n",
    "        if name.split(\"_\")[-1] in omics\n",
    "    ]\n",
    "    #print(f\"selected {len(indexes)} for omics: {omics}\")\n",
    "    return df.values[:, indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimators\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "class MyLasso(Lasso):\n",
    "    def __init__(self, C, max_iter=1000):\n",
    "        self.C = C\n",
    "        super().__init__(1.-C, max_iter=max_iter)\n",
    "    def predict(self, X):\n",
    "        return (super().predict(X) > 0.5).astype(float)\n",
    "\n",
    "estimators = {\n",
    "    \"LR\": (LogisticRegression, {'penalty':'l1', 'solver':'liblinear', 'max_iter':200000, 'random_state':0}),\n",
    "    \"SVM\": (LinearSVC, {'penalty':'l1', 'dual':False, 'max_iter':200000, 'random_state':0}),\n",
    "    \"Lasso\": (MyLasso, {'max_iter':2000})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for collecting results\n",
    "\n",
    "from typing import List, Set, Dict\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import stability_measure\n",
    "\n",
    "\n",
    "class CVResult:\n",
    "    def __init__(self, n_features:int):\n",
    "        self.n_features = n_features # number of all features\n",
    "        self.metrics = {\n",
    "            \"accuracy\": accuracy_score\n",
    "        }\n",
    "        self.results = {}\n",
    "        self.features_sets = []\n",
    "    \n",
    "    def add_feature_set(self, feature_set:Set[int]):\n",
    "        self.features_sets.append(feature_set)\n",
    "\n",
    "    def evaluate(self, prefix:str, y_real:List[float], y_predict:List[float]):\n",
    "        for name, fun in self.metrics.items():\n",
    "            key = f\"{prefix}_{name}\"\n",
    "            if key not in self.results:\n",
    "                self.results[key] = []\n",
    "            self.results[key].append(fun(y_real,y_predict))\n",
    "    \n",
    "    def summary(self)->Dict[str,float]:\n",
    "        results = {key:mean(values) for key, values in self.results.items()}\n",
    "        results[\"fss_nogueira\"] = stability_measure.nogueira(self.features_sets, self.n_features)\n",
    "        results[\"fss_lustgarten\"] = stability_measure.lustgarten(self.features_sets, self.n_features)\n",
    "        results[\"fss_jaccard_index\"] = stability_measure.jaccard_index(self.features_sets, self.n_features)\n",
    "        results[\"avg_features\"] = mean([len(s) for s in self.features_sets])\n",
    "        results[\"all_features\"] = self.n_features\n",
    "        results[\"runs\"] = len(self.features_sets)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment core\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from utils import estimator_helpers\n",
    "\n",
    "\n",
    "data = {}\n",
    "buffer_results = True\n",
    "\n",
    "def append(data:Dict[str, List[float]], key:str, value:float):\n",
    "    \"\"\"Append new value to results collection\"\"\"\n",
    "    if key not in data:\n",
    "        data[key] = []\n",
    "    data[key].append(value)\n",
    "\n",
    "# experiment parameters\n",
    "N_SPLITS = [5,]\n",
    "OMICS = [\"__ALL__\", \"cnv\", \"mirna\", \"mutation\", \"rna\"]\n",
    "RANDOM_SEEDS = [0,42,21]\n",
    "FEATURES_FRACTION = [7,30,55] # percentage of the number of objects in the train data set\n",
    "\n",
    "i = 0\n",
    "I =  len(datasets) * len(OMICS) * len(N_SPLITS)\n",
    "\n",
    "print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), \"Experiment started ...\")\n",
    "\n",
    "for ds_name, df in datasets.items():\n",
    "    print(\"------------------------------------\")\n",
    "    print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), f\"Dataset {ds_name} {df.shape}\")\n",
    "    y = df.values[:,0]\n",
    "    \n",
    "    for omics in OMICS:\n",
    "        print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), f\"Omics {omics}\")  \n",
    "        X = get_X(df.iloc[:,1:], omics)\n",
    "\n",
    "        for n_splits in N_SPLITS:\n",
    "            train_size = round((1.-1./n_splits)*X.shape[0])\n",
    "            features_nums = [round(train_size*fraction/100) for fraction in FEATURES_FRACTION]\n",
    "            selectors = estimator_helpers.create_selectors(estimators, X, y, features_nums, train_size=train_size)\n",
    "            print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), \"Selectors with estimators created:\")\n",
    "            for name, selector in selectors.items():\n",
    "                print(f\"- {name}:\", re.sub(\"\\n\\s*\", \" \", f\"{selector.estimator}\"))\n",
    "\n",
    "            results = { name: CVResult(X.shape[1]) for name in selectors }\n",
    "\n",
    "            for random_seed in RANDOM_SEEDS:\n",
    "                skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "                    \n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, y_train = X[train_index], y[train_index]\n",
    "                    X_test, y_test = X[test_index], y[test_index]\n",
    "                    \n",
    "                    for name, selector in selectors.items():\n",
    "                        # calculate results on this split for every estimator/selector\n",
    "                        result = results[name]\n",
    "                        selector.fit(X_train, y_train)\n",
    "                        selected_features_indexes = selector.get_support(indices=True)\n",
    "                        result.add_feature_set(set(selected_features_indexes))\n",
    "                        result.evaluate(\"train\", y_train, selector.estimator_.predict(X_train))\n",
    "                        result.evaluate(\"test\", y_test, selector.estimator_.predict(X_test))\n",
    "            i+=1\n",
    "            print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), f\"Results {i}/{I} for {ds_name} {omics} and {n_splits}-fold cross validation\")\n",
    "            \n",
    "            for name, result in results.items():\n",
    "                r = result.summary()\n",
    "                #print(f\"{name:5} : \", r)\n",
    "                append(data, \"cv-folds\", n_splits)\n",
    "                append(data, \"dataset\", ds_name)\n",
    "                append(data, \"estimator\", name)\n",
    "                append(data, \"estimator_details\", re.sub(\"\\n\\s*\", \" \", f\"{selectors[name].estimator_}\"))\n",
    "                append(data, \"omics\", omics_str(omics))\n",
    "                for key, value in r.items():\n",
    "                    append(data, key, value)\n",
    "\n",
    "            if buffer_results:\n",
    "                pd.DataFrame(data).to_csv(f\"results_buffer.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"results/r_{datetime.today().strftime('%Y%m%d_%H%M')}.csv\", index=False)\n",
    "print(datetime.today().strftime('%Y-%m-%d %H:%M:%S'), \"Experiment finished ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tests\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import estimator_helpers\n",
    "\n",
    "class MyLasso(Lasso):\n",
    "    def __init__(self, C):\n",
    "        self.C = C\n",
    "        super().__init__(1.-C)\n",
    "\n",
    "df = datasets[\"BLCA\"]\n",
    "y = df.values[:,0]\n",
    "X = get_X(df.iloc[:,1:], \"__ALL__\")\n",
    "\n",
    "estimator_class = LinearSVC\n",
    "estimator_params = {'penalty':'l1', 'dual':False, 'max_iter':100000}\n",
    "\n",
    "X, _, y, _ = train_test_split(X, y, train_size=(1.-1./5), random_state=42, stratify=y)\n",
    "\n",
    "#c_low = estimator_helpers.match_C_to_number_of_features(estimator_class, 31, X, y, estimator_params=estimator_params, verbose=True)\n",
    "#print(\"--->\", c_low)\n",
    "#c_mid = estimator_helpers.match_C_to_number_of_features(estimator_class, 122, X, y, estimator_params=estimator_params, verbose=True)\n",
    "#print(\"--->\", c_mid)\n",
    "c_high = estimator_helpers.match_C_to_number_of_features(estimator_class, 200, X, y, estimator_params=estimator_params, verbose=True)\n",
    "print(\"--->\", c_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
